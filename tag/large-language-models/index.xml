<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models | Mrinaal Dogra</title>
    <link>https://mrinaald.github.io/tag/large-language-models/</link>
      <atom:link href="https://mrinaald.github.io/tag/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <description>Large Language Models</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 30 Nov 2024 05:30:00 +0000</lastBuildDate>
    <image>
      <url>https://mrinaald.github.io/media/icon_hub96e8e9402b9c6d6f1cb54dd30395281_15734_512x512_fill_lanczos_center_3.png</url>
      <title>Large Language Models</title>
      <link>https://mrinaald.github.io/tag/large-language-models/</link>
    </image>
    
    <item>
      <title>Multi-task Learning with ToolkenGPT Framework</title>
      <link>https://mrinaald.github.io/project/toolkengpt/</link>
      <pubDate>Sat, 30 Nov 2024 05:30:00 +0000</pubDate>
      <guid>https://mrinaald.github.io/project/toolkengpt/</guid>
      <description>&lt;!-- &lt;h2 &gt;Details&lt;/h2&gt; --&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;This project was done as an graduate project, in the Fall &#39;24 term at UC San Diego under Prof. Lian Hui Qin, Department of Computer Science and Engineering, UC San Diego.&lt;/p&gt;
&lt;br&gt;
&lt;!-- &lt;h2&gt;Brief summary of the work done in the project&lt;/h2&gt; --&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;This project explored the capabilities of &lt;b&gt;ToolkenGPT&lt;/b&gt;, a modular framework for tool-augmented language models, by integrating it with &lt;b&gt;smaller, modern LLMs&lt;/b&gt; like &lt;b&gt;Llama 3.2&lt;/b&gt;. My focus was on enabling &lt;b&gt;multi-task learning&lt;/b&gt; within this architecture, i.e., training the model to handle diverse tasks like &lt;b&gt;numerical reasoning&lt;/b&gt; and &lt;b&gt;knowledge-based QA&lt;/b&gt; simultaneously.&lt;/p&gt;
&lt;br&gt;
&lt;h2&gt;Project Highlights&lt;/h2&gt;
&lt;ol style=&#34;text-align: justify;&#34;&gt;
  &lt;li&gt; &lt;b&gt;Framework Re-implementation&lt;/b&gt;: Adapted ToolkenGPT to work with Hugging Faceâ€™s Transformer-based Llama models. This required significant changes to the training, model, and inference pipelines to ensure compatibility and extensibility.
  &lt;li&gt; &lt;b&gt;Multi-task Learning Extension&lt;/b&gt;: Designed and implemented a new multitask training pipeline and model (&lt;code&gt;MultiTaskFunctionLM&lt;/code&gt;) to support concurrent learning across multiple tasks.
  &lt;li&gt; &lt;b&gt;Experimental Research&lt;/b&gt;: Conducted experiments to evaluate task synergy and performance trade-offs in multi-task setups. This included analyzing the effect of joint-task training strategies.
  &lt;li&gt; &lt;b&gt;Evaluation &amp; Benchmarking&lt;/b&gt;: Developed custom evaluation scripts for benchmark datasets such as &lt;b&gt;GSM8K&lt;/b&gt;, &lt;b&gt;FuncQA&lt;/b&gt;, and &lt;b&gt;KAMEL&lt;/b&gt;, measuring model generalization and reasoning performance.
&lt;/ol&gt;
&lt;br&gt;
&lt;h2&gt;Key Contributions:&lt;/h2&gt;
&lt;ol style=&#34;text-align: justify;&#34;&gt;
  &lt;li&gt; Rewrote core training and inference modules to support Hugging Face models (&lt;code&gt;train_llama.py&lt;/code&gt;, &lt;code&gt;inference_llama.py&lt;/code&gt;, &lt;code&gt;model.py&lt;/code&gt;)
  &lt;li&gt; Introduced a multitask model and training script (&lt;code&gt;multitask_model.py&lt;/code&gt;, &lt;code&gt;train_llama_multitask.py&lt;/code&gt;)
  &lt;li&gt; Built dataset converters and evaluators for task-specific benchmarks (&lt;code&gt;convert_data.py&lt;/code&gt;, &lt;code&gt;eval_gsm8k_funcqa.py&lt;/code&gt;, etc.)
&lt;/ol&gt;
&lt;br&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;&lt;b&gt;Note&lt;/b&gt;: The multitask model is still under development and currently supports inference on only the primary task in a multi-task setting due to architectural limitations.&lt;/p&gt;
&lt;br&gt;
&lt;h2&gt;Takeaway:&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;This project provided hands-on experience in &lt;b&gt;deep model engineering&lt;/b&gt;, &lt;b&gt;multi-task learning&lt;/b&gt;, and &lt;b&gt;LLM fine-tuning&lt;/b&gt;, while demonstrating how tool-augmented models can scale to support flexible reasoning across diverse tasks.&lt;/p&gt;
&lt;h3&gt;Attributions:&lt;/h3&gt;
&lt;ul style=&#34;text-align: justify;&#34;&gt;
  &lt;!-- &lt;li&gt;&lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Ada_horizon_green_logo_with_slogan.svg&#34;&gt;&#34;Ada programming language logo (2023)&#34;&lt;/a&gt; by &lt;a href=&#34;https://commons.wikimedia.org/wiki/User:Captain-Haddock17&#34;&gt;William J. Franck&lt;/a&gt; is licensed under &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/deed.en&#34;&gt;CC0 1.0&lt;/a&gt; / Merged with other images&lt;/li&gt; --&gt;
  &lt;li&gt;Image source: &lt;a href=&#34;https://github.com/Ber666/ToolkenGPT/blob/main/assets/image.png&#34;&gt;ToolkenGPT&lt;/a&gt; by &lt;i&gt;Hao et al.&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
